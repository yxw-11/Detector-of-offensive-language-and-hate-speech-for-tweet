{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9VDfC-ie19P"
   },
   "source": [
    "# Hate_Speech_and_Offensive_Language\n",
    "\n",
    "Project mentor: Carlos Aguirre\n",
    "\n",
    "Yuxiang Wang <ywang594@jh.edu>, Jingxi Liu <jliu238@jh.edu>, Wenkai Luo <wluo14@jh.edu>, Yuetong Liu<yliu390@jh.edu>\n",
    "\n",
    "https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqwI3PT-hBJo"
   },
   "source": [
    "# Outline and Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7Af6y48e7HI"
   },
   "source": [
    "### Uncompleted Deliverables\n",
    "1. \"Would like to complete #2\": Unsupervised learning: clustering hate speech to identify major topics, including race, colour, sex (ran out of time).\n",
    "2. \"Would like to complete #3\": Improvement: link data with CF users to study who are more likely to use hate speech/offensive language. Integrate the frequency of a user detected for posting hate speech and the user’s interaction with others (ran out of time).\n",
    "\n",
    "\n",
    "### Completed Deliverables\n",
    "1. \"Must complete #1\": Data Pre-processing: pre-process text data including deleting duplicates, removing stop words or punctuation, and convertingtweet content to lowercase. in \"Pre-processing\" below.\n",
    "2. \"Must complete #2\": Feature Extraction: Combine skip-gram, LIWC and LDA methods to extract features. [in Feature extractions](https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Feature_Extraction%20.ipynb)\n",
    "3. \"Must complete #3\": Feature Selection: Use PMI on the training data to quantified the importance of each feature. Set up a threshold to choose themost beneficial features. Most features share similar mutual information, so we switched Logistics Regression with L1 regulization.[in \"Classic Methods\" below](https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb).\n",
    "4. \"Expect to complete #1\": EDA: conduct exploratory data analysis. Moreover, conduct correlation analysis between features.in \"Pre-processing\" below and [in \"Classic Methods\" below](https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb)\n",
    "5. \"Expect to complete #2\": Prediction: Use the F1 value on the Davidson, T. et al. (2017) as a baseline, accurately classify the text comment into hate speech,offensive language or neither. Otherwise, interpret the model to explain the mis-classification.[in \"Classic Methods\" below](https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb)\n",
    "6. \"Expect to complete #3\": Model Selection: Besides logistic regression model, use MLP + CNN as a comparisons to train data and make model selectionbased on their performance. Use cross validation to find optimal hyper-parameters. in \"Deep Learning Methods\" below]\n",
    "7. \"Would like to complete #1\": Beside improving the precision, choose model based onevaluation metrics. [in \"Classic Methods\" below](https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb)\n",
    "\n",
    "\n",
    "### Additional Deliverables\n",
    "1. We add TFIDF as a new method to extract features in [in Feature extractions](https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Feature_Extraction%20.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import warnings\n",
    "from random import choice\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import csv\n",
    "from liwc import LIWC\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eiq2aSauhSsS"
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtWkhiIPfOfK"
   },
   "source": [
    "### What problem were you trying to solve or understand?\n",
    "Our project is to design a hate speech detector. we try to figure out the two key challenges for automatic hate speech detection on social media. The first is to decide whether a post is a hate speech. The second is to separate hate speech and other offensive languages.\n",
    "\n",
    "\n",
    "\n",
    "### What are the real-world implications of this data and task?\n",
    "\n",
    "Although the negative emotion expressions increase for internet’s lack of regulation, it makes the detection and supervision of hate speech possible to achieve by artificial intelligence. A machine learning solution could identify hate speech, and then flag it or remove it for review. At present, solution that relied human annotator is expensive and difficult for widespread use. Platform on the internet are interested in an automatic hate speech censor.\n",
    "\n",
    "### How is this problem similar to others we’ve seen in lectures, breakouts, and homeworks?\n",
    "\n",
    "The project is similar to Homework#4 of machine learning course, which is a binary classification problem of text data. We implement TFIDF and a support vector machine to classify the topic of news articles. However, it is different in this project that we analyze tweets here instead of documents and we try many combinations of methods on data extraction. The task in this project is more complex for the small size of each sample and the “less-standard English” on Twitter.\n",
    "\n",
    "\n",
    "### What makes this problem unique?\n",
    "\n",
    "The problem is unique for two reason. Firstly, there is no legal definition of hate speech in the U.S. law. But it is generally believed that the speech based on identity characteristics such as race and gender, which deliberately incites violence and prejudice, may constitute hate speech. The labels in data are given by users based on their feelings. And we want to build a classifier that is similar as natural human emotion. Secondly, Twitter data is uniquely messy and hard to preprocess. The problems includes misspelling, slang, colloquialisms,  irregular format, meaningless addition and the like.\n",
    "\n",
    "\n",
    "### What ethical implications does this problem have?\n",
    "People possess the right to express ideas and information regulated in the First Amendment. Therefore, labelling normal speech as hate speech incorrectly may break the freedom of speech. On the other hand, hate speech can causes many social problems such as racial inequality and climate of intolerance. Therefore, harmful speech left uncensored may violate the rights of others and do harm to the whole society."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFq-_D0khnhh"
   },
   "source": [
    "## Dataset(s)\n",
    "\n",
    "Describe the dataset(s) you used.\n",
    "\n",
    "### 1.How were they collected?\n",
    "\n",
    "  The data came from the statistics of users’ hate speech and offensive language in social media.\n",
    "\n",
    "### 2.Why did you choose them?\n",
    "\n",
    "  First of all, we are very interested in the topic of hate speech and offensive language detection. Secondly, this topic has good social significance and the results of this study can somehow help improve the network environment.\n",
    "\n",
    "### 3.How many examples in each?\n",
    "\n",
    "  24783 samples in total and each sample contains 6 columns including labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2lOicoBYif7g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  hate_speech  offensive_language  neither  class  \\\n",
      "0      3            0                   0        3      2   \n",
      "1      3            0                   3        0      1   \n",
      "2      3            0                   3        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n"
     ]
    }
   ],
   "source": [
    "# Load data and show some examples\n",
    "df=pd.read_csv(\"../data/labeled_data.csv\")\n",
    "df.drop(df.columns[[0]], axis=1,inplace=True)\n",
    "print(df.iloc[:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XN1fYEfGidiD"
   },
   "source": [
    "## Pre-processing\n",
    "\n",
    "1.What features did you use or choose not to use? Why?\n",
    "\n",
    "We choose to use column named \"tweet\" which is directly related to our topic and the column named \"class\" is our label column.Since our \n",
    "goal is to detect hate speech and offensive language through text analysis, we decided not to use other features which are related to the \n",
    "count of words. \n",
    "\n",
    "2.If you have categorical labels, were your datasets class-balanced?\n",
    "\n",
    "We do have categorical labels, and they are unbalanced.(Shown below)\n",
    "\n",
    "3.How did you deal with missing data? What about outliers?\n",
    "\n",
    "Fortunately, our dataset does not have missing data. Besides, what we are really insterested in are all \"string\" format data, so we do not consider outliers here. While we do deal with some of the tweets that are problematic. For example, some sentences only contain special punctuations, and some sentences have # before every word.\n",
    "\n",
    "4.What approach(es) did you use to pre-process your data? Why?\n",
    "\n",
    "We remove less important part of each tweet:<br>\n",
    "1 Punctuation <br>\n",
    "2 Stop words <br>\n",
    "3 Hashtag <br>\n",
    "4 Username <br>\n",
    "5 Http information <br>\n",
    "6 Digit and excessive whitespaces <br>\n",
    "\n",
    "Then we also transform all the letters into lower case, and then do stemming and tokenization. In particular, we deal with those special problematic sentences by rewriting regular expression for them.\n",
    "\n",
    "All of the operations above are in order to retain only important information of tweet and remove irrelevant information. Let each of our data maintains words which are related to distinguishing hate speech and offensive language. Finally, our data would be more accurate in word embedding part, which may help the prediction of our models.\n",
    "\n",
    "5.Are your features continuous or categorical? How do you treat these features differently?\n",
    "\n",
    "Our data is text format, so after embedding all the tweets we only have continuous numeric vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EEuKEzM5ipag"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0 1430\n",
      "label 1 19190\n",
      "label 2 4163\n"
     ]
    }
   ],
   "source": [
    "# For those same examples above, what do they look like after being pre-processed?\n",
    "print(\"label 0\",np.sum(df[\"class\"]==0))\n",
    "print(\"label 1\",np.sum(df[\"class\"]==1))\n",
    "print(\"label 2\",np.sum(df[\"class\"]==2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus our data is obviously unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average len before preprocessing 85.43606504458701\n"
     ]
    }
   ],
   "source": [
    "# Tweet length before preprocessing\n",
    "#average len of tweet\n",
    "tweet = np.array(df[\"tweet\"])\n",
    "avg=0\n",
    "num=[]\n",
    "x=[x for x in range(24783)]\n",
    "for n in range(tweet.shape[0]):\n",
    "    avg+=len(tweet[n])\n",
    "    num.append(len(tweet[n]))\n",
    "avg=avg/(tweet.shape[0])\n",
    "plt.bar(x,num)\n",
    "plt.show()\n",
    "print(\"average len before preprocessing\",avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Yankees\n"
     ]
    }
   ],
   "source": [
    "# Index of tweet only contain format like\"#XX #XX\" (which would be empty after remove hashtag)\n",
    "# We need to specially deal with them\n",
    "special_index=[804,826,846,848,849,923,1016,1122,1909,3398,4818,5711,6098,6279,6332,6668,7168,11951,15859,18062]\n",
    "# for example\n",
    "print(df['tweet'][804])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@TNKidsFoodPorn @Oreo ..............&#128530;\n",
      "@hoes &#9829;\n"
     ]
    }
   ],
   "source": [
    "# Also there are two meaningless tweet\n",
    "print(df['tweet'][4828])\n",
    "print(df['tweet'][6098])\n",
    "meaningless_index=[4828,6098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy original text\n",
    "txt=df.iloc[:,4:6]\n",
    "tweet=txt.iloc[:,1].copy(deep=True)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet_,specail_index,meaningless_index):\n",
    "    #remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    other = [\"#ff\", \"ff\", \"rt\"]\n",
    "    stopwords.extend(other)   \n",
    "    for i in range(tweet_.shape[0]):\n",
    "        text=tweet_[i].lower()\n",
    "        text1=''.join([word+\" \" for word in text.split() if word not in stopwords])\n",
    "        tweet_[i]=text1\n",
    "    \n",
    "    #remove digit and excessive whitespace\n",
    "    #remove url mention and hashtag\n",
    "    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    \n",
    "    for i in range(tweet_.shape[0]):\n",
    "        text_string=tweet_[i]\n",
    "        parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "        parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "        parsed_text = re.sub(mention_regex, '', parsed_text) \n",
    "        if i not in specail_index:\n",
    "            parsed_text = re.sub(hashtag_regex, '', parsed_text)\n",
    "        else:\n",
    "            parsed_text = re.sub('#','',parsed_text)\n",
    "        if i in meaningless_index:\n",
    "            parsed_text ='This is a simple tweet'\n",
    "        tweet_[i]=parsed_text\n",
    "    \n",
    "    #tokenize and stemming\n",
    "    \"\"\"Removes punctuation and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    for i in range(tweet_.shape[0]):\n",
    "        #tweet_stem=tweet_[i]\n",
    "        #tweet_stem = \" \".join(re.split(\"[^a-zA-Z]*\", tweet_stem)).strip()\n",
    "        #tweet_process = [stemmer.stem(t) for t in tweet_stem.split()]\n",
    "        #tweet_[i]=tweet_process\n",
    "        \n",
    "        tweet_stem=tweet_[i]\n",
    "        \n",
    "        remove = str.maketrans('','',string.punctuation) \n",
    "        tweet_stem = tweet_stem.translate(remove).strip()\n",
    "\n",
    "        tweet_process = [stemmer.stem(t) for t in tweet_stem.split()]\n",
    "        \n",
    "        tweet_[i]=tweet_process\n",
    "        \n",
    "        #tweet_[i] = ' '.join([str(elem) for elem in tweet_process]) \n",
    "        \n",
    "    return tweet_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [woman, complain, clean, hous, amp, man, alway...\n",
      "1    [boy, dat, coldtyga, dwn, bad, cuffin, dat, ho...\n",
      "2    [dawg, ever, fuck, bitch, start, cri, confus, ...\n",
      "3                                 [look, like, tranni]\n",
      "4    [shit, hear, might, true, might, faker, bitch,...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#results of pre-process\n",
    "tweet=preprocess(tweet,special_index,meaningless_index)\n",
    "print(tweet[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average len after preprocessing 7.689262801113666\n"
     ]
    }
   ],
   "source": [
    "# Visualize the distribution of your data before and after pre-processing.\n",
    "#   You may borrow from how we visualized data in the Lab homeworks.\n",
    "# Tweet length after preprocessing\n",
    "avg=0\n",
    "num=[]\n",
    "x=[x for x in range(24783)]\n",
    "for n in range(tweet.shape[0]):\n",
    "    avg+=len(tweet[n])\n",
    "    num.append(len(tweet[n]))\n",
    "avg=avg/(tweet.shape[0])\n",
    "plt.bar(x,num)\n",
    "plt.show()\n",
    "print(\"average len after preprocessing\",avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tASjmmtjiwvu"
   },
   "source": [
    "# Models and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlrwR9E1hnQ3"
   },
   "source": [
    "## Experimental Setup\n",
    "\n",
    "### How did you evaluate your methods? Why is that a reasonable evaluation metric for the task?\n",
    "The F1-score is used to evaluate the performance of our model. F1-score is generally used to measure the performance of classification. F1-score can be computed by the following formula: \n",
    "\\begin{equation*}\n",
    "    F_1 = 2 \\cdot \\frac{\\textbf{precision} \\cdot \\textbf{recall}}{\\textbf{precision} + \\textbf{recall}}\n",
    "\\end{equation*}\n",
    "where precision and recall can be computed by following:\n",
    "\\begin{align*}\n",
    "    \\textbf{precision} &= \\frac{TP}{TP + FP} \\\\\n",
    "    \\textbf{recall} &= \\frac{TP}{TP + FN} \\\\\n",
    "\\end{align*}\n",
    "where $TP$, $FP$ and $FN$ are the number of true positive, false positive and false negative respectively. F1-score is suitable for the classification task, especially for the unbalanced dataset, while the total accuracy for the whole classification cannot reflect the performance of model. The dataset we use for our project is an unbalanced dataset, where the hate speech samples are relatively less than the offensive samples and neither samples. Due to the unbalanced property of the dataset, even though we predict all the hate-speech as offensive or neither, it won’t affect the total accuracy too much, however the actual performance of hate speech detection can be really bad. The F1- score also take the false negative part into account which we can see the performance of classifier in the specific class. Thus F1-score is suitable evaluation metric to measure performance of model in this project.\n",
    "\n",
    "\n",
    "### What did you use for your loss function to train your models? Did you try multiple loss functions? Why or why not?\n",
    "\n",
    "For the logistic regression with L2 regularization, the loss function is the log loss, \n",
    "which can be written in the following form:\n",
    "\n",
    "\\begin{equation*}\n",
    "L = E_w(Y,x) + E_w(w)    \n",
    "\\end{equation*}\n",
    "Where \n",
    "\\begin{align*}\n",
    "E_w(x) &=   -\\sum_{i=1}^N \\left[y_i log(\\sigma({\\bf Wx})) + (1-y_i) log(1 -{\\bf Wx}) \\right]\\\\\n",
    "E_w(w) &=   - \\lambda \\sum_{c = 1}^C ||\\vec{w}_c||_2^2 \\\\\n",
    "\\sigma({\\bf Wx}) &= \\frac{1}{1+ e^{-{\\bf W^{T} x}}} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "For the TextCNN model, the loss function is also cross entropy, which is same as the log loss in logistic regression. We didn’t try different loss functions, because we consider the feature generation is the more important part for the performance of our model. We just used the common loss function in this project.\n",
    "\n",
    "### How did you split your data into train and test sets? Why?\n",
    "For the classic method, we just utilize the skit-lean build-in libraries to split our dataset randomly into different portions such as [80% Training data, 20% Testing data] and [90% Training data, 10% Testing data]. \n",
    "\n",
    "For the TextCNN, we did try to put more data into training set in order to build a more comprehensive vocabulary dictionary that incorporate as much words as possible, because we will apply word embedding to our sentence in order to represent the sentence mathematically. If size of vocabulary dictionary is too small, the out-of-vocabulary issue will introduce the sparsity into the embedding matrix, which might cause overfitting.\n",
    "\n",
    "But we have to admit that it might not be a good idea to split our data in this way due to the unbalanced property of the datasets. It might introduce the bias into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git repo for feature extractions:\n",
    "https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Feature_Extraction%20.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUNxC358jPDr"
   },
   "source": [
    "### Code for loss functions, evaluation metrics or link to Git repo:\n",
    "https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMyqHUa0jUw7"
   },
   "source": [
    "## Baselines \n",
    "\n",
    "### What baselines did you compare against? Why are these reasonable?\n",
    "The baseline mode we choose is the one that provide the dataset in [Automated Hate Speech Detection and the Problem of Offensive Language](http://sdl.soc.cornell.edu/img/publication_pdf/hatespeechdetection.pdf). This model created a pipeline of hate speech detection, which also incorporate most of state-of-art feature generation methods. Also, this baseline model also demonstrates the explicit performance of model in specific hate speech class with the use of F1-score, while other paper might only compare the weighted-average F1-score for all three class. Our classic method using logistic regression and SVM utilize some of the ideas from this baseline while also introduce the embedding feature as our contribution. For the TextCNN model, we aim at exploring the effect of different environment setting to the performance of hate speech classification.\n",
    "\n",
    "### Did you look at related work to contextualize how others methods or baselines have performed on this dataset/task? If so, how did those methods do?\n",
    "The difficult of hate-speech detection are the its similarity with offensive language, even some paper consider hate-speech as a subset of offensive language, and its subjectivity, which is also demonstrated in the dataset. [[1]](http://sdl.soc.cornell.edu/img/publication_pdf/hatespeechdetection.pdf).\n",
    "\n",
    "Hate speech is a interested area where current research can be generally divided into two categories including the classic machine learning method such as logistic method and deep neural network such as TextCNN, LSTM and RNN [[2]]( 2Q2WDCB638D43FE1E1190A9E8DCB8ED30CF459E7560D_unknown_A33959A0DF385E32CF856B03ABAA940EF1C0B61C_4). The classic mostly relies on the feature created from the text file. Several paperd investigate effect of different features, however they just stack the features together and do feature selection to boost the performance. The popular features include PoS, n-gram and so on. The neural network will automatically select the feature by the network. Different neural network including RNN, CNN, LSTM has shown that they have promising future in this application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqB48IF9kMBf"
   },
   "source": [
    "## Classic Methods\n",
    "\n",
    "### What methods did you choose? Why did you choose them?\n",
    "Based on the prior work and literature review. Both logistic regression and SVM seem promising methods for this classifying application.In this project, we will be focusing on logistic regression with L2 regularization. A one-versus-rest framework will be applied in thisproject where each classifier will be trained separately for the specific class. During the testing stage, the class with the highest predictedprobability will be labelled for the corresponding sample.\n",
    "\n",
    "### How did you train these methods, and how did you evaluate them? Why?\n",
    "We trained this models and evaluated them based on F1_mirco_score. We use micro score because the data is inbalanced, and we are more interested in performance of the minor group(hate speech)\n",
    "\n",
    "### Which methods were easy/difficult to implement and train? Why?\n",
    "The model is easy to implement. However, it takes long time to train due to the large dimension of the sparse features.\n",
    "\n",
    "### For each method, what hyperparameters did you evaluate? How sensitive was your model's performance to different hyperparameter settings?\n",
    "For logistic Regression, we tuned regularization strength C and tried several optimization algorithms. The performance doesn't have very significant change after hyperparameter tuning, but the accuracy does increase.\n",
    "\n",
    "### Code for training models, or link to your Git repository:\n",
    "https://github.com/YuetongLiu/Hate_Speech_and_Offensive_Language/blob/main/code/Logistic_n_SVM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqB48IF9kMBf"
   },
   "source": [
    "## Deep Learning Methods\n",
    "\n",
    "### What methods did you choose? Why did you choose them?\n",
    "\n",
    "In order to investigate the performance using different methods, we plan to use TextCNN to tackle the same problem in this project to investigate the difference between classic method and neural network. The motivation behind TextCNN is that the combination of Multilayer Perceptron (MLP) and Convolutional Neural Networks (CNN) did a really good job in image classification, and TextCNN has been applied in this field for a while. As a group that are new to machine learning, we decided to choose the method that we are able to handle and modified. TextCNN is familiar to us and easier to implement, which also provide us with flexibility to modify the network by ourselves.\n",
    "\n",
    "### How did you train these methods, and how did you evaluate them? Why?\n",
    "\n",
    "To design and train out TextCNN model, we utilize the pytorch framework and torchtext package.  We design the TextCNN based on the [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) and use torchtext to create the dataset using the preprocessed data. In term of training, we use opt.Adam to optimize the object function and update the parameters in the network. Adam is able to adjust the learning rate based on the curvature of the cost function. Backpropagation is the theoretical basis for neural network training. The cross entropy will be back propagated into initial layer to be used to update the parameter. In addition, we also apply over-fitting to prevent overfitting in the training process. The evaluation metric is same as logistic regression, which has been discussed in the previous sessions.\n",
    "### Which methods were easy/difficult to implement and train? Why?\n",
    "The TextCNN model is not hard to train and implement, but we did spent time to investigate the suitable setting for this specific dataset and be familiar with the package and framework. Apart from the training environment, the data processing is also an important part, which will affect the training process and model performance. \n",
    "\n",
    "To find a suitable pre-processing criterion that can be matched with the methods we are going to use, we also need to need to investigate the property of the text including the significant part of the tweets such as url, hashtage and username and the length distribution of the dataset. This part has already been discussed in the pre-processing part. \n",
    "\n",
    "Due to the diversity of the tweet, the length of sentence after pre-processing might have a significant difference, some tweet might only contain one or two words, which will introduce the sparsity into the model. We need to take that in to account, otherwise it might cause over-fitting. Thus we need to find a suitable setting to avoid this issue.\n",
    "\n",
    "### For each method, what hyperparameters did you evaluate? How sensitive was your model's performance to different hyperparameter settings?\n",
    "\n",
    "There are three hyperparameters that we think important in the TextCNN, which are kernel size, embedding dimension and sentence length.\n",
    "\n",
    "For the kernel size, doing the convolution in embedding matrix of sentence is similar like computing the n-gram feature from sentences. Based on that fact that our data doesn’t have long sentence and paragraph, we try different kernel size. Eventually we use [1,2,3] as our model convolution layer kernels size. Theoretically, it is computing the one-gram, bi-gram and tri-gram. \n",
    "\n",
    "For the embedding dimension, it’s the key hyperparameter in the TextCNN model, because It decided the performance of model. As we discussed in SVM lecture. We believe we can project our nonlinear data into high dimensional space to find a linearly separable hyperplane. Embedding is like project the text data into a word vector representation form, which is able to be recognised by our model. In this project we didn’t introduce external embedding method, but let the model learn the embedding during training. A suitable embedding dimension is from 100~300, which is good enough to distinguish each word in the builded vocabulary dictionary. We use the torchtext data.Field to handle this task, which also allow the system embed unknown word in to specific word vector to tackle out-of-vocabulary issue. \n",
    "For sentence length, it is also important for the performance of model. Our strategy is to keep all the sentence within a fixed length. For those are less than the desired length, we will pad the sentence until it matches with the requirement, and truncate those exceed the desired length. When we investigate the property of the data we have.  We realized that this dataset is a messy raw dataset with lots of username, hashtag, url, emoji and so on. Although these kinds of information might be helpful for model to predict the correct class, they are useless at most of time. Thus, we apply rigour pre-processing criterion to our dataset, this introduce the a side-effect into the model. We realized that there some sample only contain one or two words, where it might introduce huge sparsity into the model. Thus, we need to find the suitable desired length. After we investigate the length distribution of text after pre-processing. We find 15 is a suitable desired length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for training models, or link to your Git repository:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Zdp4_H-kx8H"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./T1.png)\n",
    "![jupyter](./T2.png)\n",
    "![jupyter](./textcnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bS2sjfbglG_V"
   },
   "source": [
    "### What about these results surprised you? Why?\n",
    "\n",
    "### Did your models over- or under-fit? How can you tell? What did you do to address these issues?\n",
    "For TextCNN model, the test accuracy is increasing while the train accuracy is also increasing, thus we can say there isn't a overfitting in the TextCNN model.\n",
    "### What does the evaluation of your trained models tell you about your data? How do you expect these models might behave differently on different data?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59EbS1GilSQ_"
   },
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugJXhZKNlUT4"
   },
   "source": [
    "## What you've learned\n",
    "\n",
    "We used lots of concepts that we learned from class. Lectures on SVM and logistic regression are extremely helpful when we are dealing with this classification problem. Moreover, the homework on kernel functions gave us inspriation on extracting numrical features for natural language. Also, the CNN homework helped us to be familiar with the structure of CNN, so than make it easier when we building TextCNN.\n",
    "\n",
    "Since many people have worked on this proejct before, we have many resource for literature review. It saved us many time to figure out the pipeline of this project, and give us some new aspects when approaching the problem. I think literature review would be important for us when working for other projects.\n",
    "\n",
    "After the presentation, other groups gave us some suggestios on data pre-processing, including remove non-essential words. After removing more noising words from our dataset, we are able to extract features which are more relevant to the topic.\n",
    "\n",
    "Even though we tried several feature extraction methods, not all of them have good performance when fitting into the model. If we could have more time to work on this project, we would want to evaluate the relevance between features and our raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final Project Submission Template",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
